import torch
import torch.nn.functional as F
import torch.nn as nn
import math
from torch.optim import Adam
from torch_geometric.nn import GCNConv, GATConv, GINConv
from torch_geometric.datasets import Planetoid
from torch_geometric.loader import DataLoader
from torchmetrics.classification import Accuracy, AUROC, CohenKappa, ConfusionMatrix, F1Score, Precision, RecallAtFixedPrecision, Recall, PrecisionRecallCurve

from Helper_functions import calculate_metrics, FocalLoss
#%% Model wrapper
class ModelWrapper:
    def __init__(self, model, optimizer, criterion, use_amp=None):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        if use_amp is None:
            enable_amp = torch.cuda.is_available() and not isinstance(criterion, FocalLoss)
        else:
            enable_amp = bool(use_amp)
        self._use_amp = enable_amp
        self._scaler = _make_scaler(enabled=self._use_amp)
        
    def train_step(self, data, mask):
        self.model.train()
        self.optimizer.zero_grad()
        with _autocast(enabled=self._use_amp):
            out = self.model(data)
            loss = self.criterion(out[mask], data.y[mask])
        
        if torch.isnan(loss):
            raise ValueError("Loss is NaN, stopping training")

        loss_value = float(loss.detach())
        if self._use_amp:
            self._scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self._scaler.step(self.optimizer)
            self._scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
        return loss_value
    def evaluate(self, data, mask):
        self.model.eval()
        with torch.no_grad():
            with _autocast(enabled=self._use_amp):
                out = self.model(data)
                loss = self.criterion(out[mask], data.y[mask])
            pred = out.argmax(dim=1)
            probs = torch.nn.functional.softmax(out, dim=1)
        metrics = calculate_metrics(data.y[mask].cpu().numpy(), pred[mask].cpu().numpy(), probs[mask].cpu().numpy())
        return float(loss.detach()), metrics
    def get_loss(self, data, mask):
        self.model.train()
        with _autocast(enabled=self._use_amp):
            out = self.model(data)
            loss = self.criterion(out[mask], data.y[mask])
        
        if torch.isnan(loss):
            raise ValueError("Loss is NaN, stopping training")
        return loss
    
try:
    from torch.amp import autocast as _torch_autocast
    from torch.amp import GradScaler as _TorchGradScaler

    def _autocast(enabled: bool):
        return _torch_autocast("cuda", enabled=enabled)

    def _make_scaler(enabled: bool):
        return _TorchGradScaler("cuda", enabled=enabled)
except (ImportError, TypeError, AttributeError):
    from torch.cuda.amp import autocast as _torch_autocast  # type: ignore
    from torch.cuda.amp import GradScaler as _TorchGradScaler  # type: ignore

    def _autocast(enabled: bool):
        return _torch_autocast(enabled=enabled)

    def _make_scaler(enabled: bool):
        return _TorchGradScaler(enabled=enabled)


#%% models

class GCN(torch.nn.Module):
    """
    A simple Graph Convolutional Network model.
    """
    def __init__(self, num_node_features, num_classes, hidden_units, dropout=0.5):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(num_node_features, hidden_units)
        self.conv2 = GCNConv(hidden_units, num_classes)
        self.dropout = dropout

    def forward(self, data):
        # x: Node features [num_nodes, in_channels]
        # edge_index: Graph connectivity [2, num_edges]
        x = data.x
        edge_index = data.edge_index

        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        
        # Output raw logits
        x = self.conv2(x, edge_index)
        return x

class GAT(nn.Module):
    def __init__(self, num_node_features, num_classes, hidden_units, num_heads):
        super(GAT, self).__init__()
        # Keep the total latent size roughly equal to hidden_units while limiting per-head width
        per_head_dim = max(1, math.ceil(hidden_units / num_heads))
        total_hidden = per_head_dim * num_heads
        self.conv1 = GATConv(num_node_features, per_head_dim, heads=num_heads, dropout=0.6)
        self.conv2 = GATConv(total_hidden, num_classes, heads=1, concat=False, dropout=0.6)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = self.conv2(x, edge_index)
        return x
    
class GIN(nn.Module):
    def __init__(self, num_node_features, num_classes, hidden_units):
        super(GIN, self).__init__()
        nn1 = nn.Sequential(
            nn.Linear(num_node_features, hidden_units),
            nn.ReLU(),
            nn.Linear(hidden_units, hidden_units)
        )
        self.conv1 = GINConv(nn1)

        nn2 = nn.Sequential(
            nn.Linear(hidden_units, hidden_units),
            nn.ReLU(),
            nn.Linear(hidden_units, hidden_units)
        )
        self.conv2 = GINConv(nn2)
        self.fc = nn.Linear(hidden_units, num_classes)

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        x = self.fc(x)
        return x
    
class MLP(nn.Module):
    def __init__(self, num_node_features, num_classes, hidden_units, dropout=0.5):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(num_node_features, hidden_units)
        self.fc2 = nn.Linear(hidden_units, hidden_units)
        self.fc3 = nn.Linear(hidden_units, num_classes)
        self.dropout = dropout

    def forward(self, data):
        x = data.x  # only use node features, no graph structure
        x = F.relu(self.fc1(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.relu(self.fc2(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.fc3(x)
        return x